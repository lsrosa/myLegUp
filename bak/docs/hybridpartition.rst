.. highlight:: cpp

.. _partition:

Hybrid Partition Analysis
==========================

LegUp provides not only pure hardware flow, but also hardware/software hybrid flow.
This section is to introduce one of the steps in hybrid design flow, suggesting
candidate functions to be accelerated. At the end of this step, users can find
software cycle counts and estimated hybrid cycle counts of the functions.

Software Cycles
---------------
Software cycles can be gathered by running the program on a soft MIPS processor
with a hardware profiler monitoring the execution. Other than actually executing
the C program on the soft processor on FPGA, we also provide a simulation flow
to gather the profiling data. Note that currently both the on-board-execution and simulation
flows support only DE2 board, so please make sure the variable `FPGA_board` 
in example/Makefile.config is set to `DE2` instead of `DE4`.
To run the flows for other devices, some minor changes are required.

Profile on FPGA
++++++++++++++++
To profile a C program's actual execution on the soft processor, the make target
to use is `profile_tiger_on_board`. To eliminate the print functions (printf),
run the make target with variable `PRINTF_OFF` defined, e.g.,

.. code-block:: make

    make profile_tiger_on_board PRINTF_OFF=elimatating_print_function.

.. code-block:: make

	profile_tiger_on_board: tiger_with_profiler
		make pre_on_board_profiling
		make on_board_profiling
		make post_on_board_profiling

The `tiger_with_profiler` make target compiles the C programs to executable
and generates byte code to be downloaded to sdram on the FPGA board. It also
generates the hash parameters that are used to configure the profiler, by calling
another make target `gen_hash_for_profiler`.

Then the `pre_on_board_profiling` make target prepares the files (\*.dat) in 
on_board directory. These files contains the program's instruction byte code as
well as the hash parameters. These contents will be downloaded to the FPGA board
(SDRAM or the FPGA Chip) from your host computer.

Next, the `on_board_profiling` make target programs the FPGA and uses `system-console`
to run a tcl file named `profile.tcl`. This tcl file downloads the program's
instruction and hash parameters to FPGA, starts program's execution and retrieves
profiling data when program finishes exection.
The profiling data retrieved from profiler are saved in on_board directory.
The profiler can be configured with
2 profiling types and 5 profiling targets. The profiling types can be hierarchical or
flat. The profiling targets can be 1) number of instructions, 2) execution cycles,
3) stall cycles, 4) instruction stall cycles and 5) data stall cycles. Currently
`on_board_profiling` profiles the program with all the 10 configurations (2*5). Please
refer to `Makefile.common` for more detail.

Finally, `post_on_board_profiling` organizes the profiling data into a report
file named `$(NAME).profiling.rpt` in the on_board directory.

The programming bit stream used in this flow is pre-compiled and targetting
the Cyclone-II FPGA chip (EP2C35F672C6). The programming bit stream is
generated by compiling the Quartus project
located at `$LEGUP_DIR/tiger/processor/tiger_DE2`.
The pre-compiled bit stream is located at `$LEGUP_DIR//tiger/tool_source/profiling_tools/tiger_leap.sof`.
	
Profile through Simulation
++++++++++++++++++++++++++
To simulation the program's execution, run `make tigersim PROFILER_ON=TRUE`.
Define varaible `PRINTF_OFF` to eliminate the print functions.
Define varaible `SHOW_PERCENTAGE` to display percentage information in generated
report files (It does not make sence to show percentage in flat profiling).
To configure the profiling type, define `DO_HIER=1` to enable hierarchical
profiling and `DO_HIER=0` to do flat profiling. The default profiling type
is hierarchical profiling.
To configure the profiling target, define the variable `CNT_INC` with corresponding
values,

.. code-block:: make

   CNT_INC=00001    # number of instructions
   CNT_INC=00010    # execution cycles (DEFAULT)
   CNT_INC=00100    # stall cycles (sum of next two)
   CNT_INC=01000    # stall cycles spent on fetching instruction
   CNT_INC=10000    # stall cycles spent on fetching data

For example, the command 

.. code-block:: make

   make tigersim PROFILER_ON=true PRINTF_OFF=true DO_HIER=1 CNT_INC=00100 SHOW_PERCENTAGE=true

hierarchically profiles the program's total cache stall cycles and adds
percentage information in the generated report file.

Hybrid Cycles
-------------
Methodology
++++++++++++++++++
The runtime (hybrid cycles) of an accelerator in hybrid system consists of 4 major
components, software wrapper cycles, hardware cycles, cycles to load data from
global memory, and cycles to store data to global memory. Refer to slide.

Detail Implementation
++++++++++++++++++++++
This section goes through the hybrid cycles prediction flow for a function.
The corresponding make target of this flow is `predictHybridCycle`:

.. code-block:: make

	# predict hybrid cycle of the first function specified in config.tcl
	predictHybridCycle:
		$(MAKE) \
		printf_off PRINTF_OFF=eliminating_print_function \
		hybridFrontend \
		\
		predictHwOnlyRenamePass \
		hybridHWloweringLinking \
		predictNonAcceleratedOnlyPass \
		predictLoweringLinking \
		predictEmulTrace \
		predictInsertTrack \
		\
		hybridSwOnlyPass \
		hybridCompileCwrapper \
		hybridSWloweringLinking \
		hybridMIPSbackend \
		hybridMIPSbinUtils \
		\
		hybridVerilogbackend \
		\
		printf_on \
		predictStateTrace \
		predictDataCycle \
		predictInstrCycle \
		predictCollectData

Each make target used in `predictHybridCycle` is described below:

.. code-block:: make

	printf_off:	
	ifdef PRINTF_OFF
		cp $(LEVEL)/../tiger/tool_source/lib/no_uart_h $(LEVEL)/../tiger/tool_source/lib/uart.h
	else
		cp $(LEVEL)/../tiger/tool_source/lib/uart_h $(LEVEL)/../tiger/tool_source/lib/uart.h
	endif

`printf_off` replace library file uart.h with no_uart_h in order to remove all
printf functions. It allows more accurate profiling and prediction by eliminating
all printf functions.

.. code-block:: make

	#run front end to produce LLVM IR
	hybridFrontend:
		# produces pre-link time optimization binary bitcode: $(NAME).prelto.bc
		$(FRONT_END) $(NAME).c -emit-llvm -pthread -c $(CFLAG) -mllvm -inline-threshold=-100 -o $(NAME).prelto.1.bc -I $(LEVEL)/../tiger/tool_source/lib
		$(LLVM_HOME)llvm-dis $(NAME).prelto.1.bc
		$(LLVM_HOME)opt -legup-config=config.tcl $(OPT_FLAGS) -legup-parallel-api < $(NAME).prelto.1.bc > $(NAME).prelto.bc
		$(LLVM_HOME)llvm-dis $(NAME).prelto.bc

`hybridFrontend` runs LLVM front end to produce pre link time optimization IR
``$(NAME).prelto.bc``. This make target is also used as the first step in hybrid
flow (make target `hybrid`).

.. code-block:: make

	# run hw-only-rename pass for hybrid
	# this pass renames the descendents of accelerating function and set linkage for global variables
	predictHwOnlyRenamePass: hybridHwOnlyPass
		$(LLVM_HOME)opt -legup-config=config.tcl $(OPT_FLAGS) -legup-hw-only-rename < $(NAME).prelto.hw.bc > $(NAME).prelto.hw_rename.bc
		mv $(NAME).prelto.hw_rename.bc $(NAME).prelto.hw.bc
		$(LLVM_HOME)llvm-dis $(NAME).prelto.hw.bc

`predictHwOnlyRenamePass` runs `hybridHwOnlyPass` as the first step.
`hybridHwOnlyPass` strips away non-accelerated functions in ``$(NAME).prelto.bc``
and saves the new IR in ``$(NAME).prelto.hw.bc``. The second step uses LLVM opt
command to run a pass called ``-legup-hw-only-rename``. This pass

* sets the linkages of all the global variables to ``LinkOnceAnyLinkage`` so that the linking between HW-side and SW-side can be performed in `predictLoweringLinking`.
* renames the descendents of the accelerating function to maintain 2 sets of function definition in the IR ``$(NAME).sw.bc`` (produced in `predictLoweringLinking`). This ensures the correct counting of basic block executions (`predictStateTrace`).

The pass is implemented in ``RenameHwOnly.cpp``. Also, ``$(NAME).prelto.hw.bc`` is
replaced by ``$(NAME).prelto.hw_rename.bc`` at the last step.

.. code-block:: make

	#Lower HW IR and link
	hybridHWloweringLinking:
		# HW part
		# performs intrinsic lowering so that the linker may be optimized
		$(LLVM_HOME)opt $(OPT_FLAGS) -legup-prelto < $(NAME).prelto.hw.bc > $(NAME).hw.lowered.bc
		# produces $(NAME).bc binary bitcode and a.out shell script: lli $(NAME).bc
		$(LLVM_HOME)llvm-ld $(LDFLAG) $(NAME).hw.lowered.bc $(LEVEL)/lib/llvm/liblegup.a $(MIPS_LIB)/libuart.a -b=$(NAME).hw.bc
		$(LLVM_HOME)llvm-dis $(NAME).hw.bc

`hybridHWloweringLinking` is also used in regular hybrid flow. In prediction flow,
this target produces a new IR ``$(NAME).hw.bc`` based on the renamed version of
HW-side IR ``$(NAME).prelto.hw.bc``. This step makes sure the IR ``$(NAME).hw.bc``
in prediction flow has the same implementation as that in regular hybrid flow.
``$(NAME).hw.bc`` will be used to 1) link back with SW-side IR in
`predictLoweringLinking` and 2) generate HW logic (Verilog) in `hybridVerilogbackend`.

.. code-block:: make

	# run non-accelerated-only pass for hybrid
	# this pass remove only the accelerated function from $(NAME).prelto.bc
	predictNonAcceleratedOnlyPass:
		$(LLVM_HOME)opt -legup-config=config.tcl $(OPT_FLAGS) -legup-non-accelerated-only < $(NAME).prelto.bc > $(NAME).prelto.sw.bc
		$(LLVM_HOME)llvm-dis $(NAME).prelto.sw.bc

`predictNonAcceleratedOnlyPass` uses LLVM opt command to run a pass called
``-legup-non-accelerated-only``. This pass is similar to the pass ``-legup-sw-only``
which is used in regular hybrid flow (`hybridSwOnlyPass`). It simply removes the
accelerating function without substitute in a wrapper function. The produced IR
is saved in ``$(NAME).prelto.sw.bc`` which is the so called SW-side IR.

.. code-block:: make

	#lower the SW IR(w/o wrapper) and link with \*.hw.bc for prediction purpose
	predictLoweringLinking:
		$(LLVM_HOME)opt $(OPT_FLAGS) -legup-prelto $(NAME).prelto.sw.bc -o $(NAME).sw.lowered.bc
		# link with hw part
		$(LLVM_HOME)llvm-ld -disable-inlining -disable-opt $(NAME).sw.lowered.bc $(NAME).hw.bc $(LEVEL)/lib/llvm/liblegup.a -b=$(NAME).sw.bc
		$(LLVM_HOME)llvm-dis $(NAME).sw.bc

`predictLoweringLinking` links back HW-side IR ``$(NAME).hw.bc`` and SW-side IR
``$(NAME).sw.lowered.bc``. The generated IR ``$(NAME).sw.bc`` is a complete but
manipulated implementation of the original program.

.. code-block:: make

	# generates executable of the IR generated by predictLoweringLinking
	# execute the program by gxemul and direct gxemul output to $(ACCELERATOR_NAME).raw.trace
	# this raw trace will be used in predictDataCycle and predictInstrCycle
	predictEmulTrace:
		# pass -legup-num-params reports # of arguments of each function
		$(LLVM_HOME)opt $(OPT_FLAGS) -legup-num-params < $(NAME).sw.bc > /dev/null
		grep $(ACCELERATOR_NAME) num_params.legup.rpt > $(ACCELERATOR_NAME).num_params.rpt
		$(LLVM_HOME)llvm-ld $(LDFLAG) -disable-inlining -disable-opt $(NAME).sw.bc $(LEVEL)/lib/llvm/liblegup.a -b=$(NAME).emul.bc
		$(LLVM_HOME)llc $(NAME).emul.bc -march=mipsel -relocation-model=static -mips-ssection-threshold=0 -mcpu=mips1 -o $(NAME).s
		$(LLVM_HOME)llvm-dis $(NAME).emul.bc
		$(MIPS_PREFIX)as $(NAME).s -mips1 -mabi=32 -o $(NAME).o -EL
		$(MIPS_PREFIX)ld -T $(MIPS_LIB)/prog_link_emul.ld -e main $(NAME).o -o $(NAME).emul.elf -EL -L $(MIPS_LIB) -lgcc -lfloat -luart_el_sim -lmem_el_sim
		$(MIPS_PREFIX)objdump -D $(NAME).emul.elf > $(NAME).emul.src
		#############################################
		##   Please type "quit" to end simulation  ##
		#############################################
		gxemul -E oldtestmips -e R3000 $(NAME).emul.elf -p `$(MIPS_LIB)/../find_ra $(NAME).emul.src` -v -T -i > $(ACCELERATOR_NAME).raw.trace

`predictEmulTrace`

* runs a pass -legup-num-params to report # of arguments of each function
* produces the executable ``$(NAME).emul.elf`` for IR ``$(NAME).sw.bc``
* emulates the program using GXemul and direct emulation output to ``$(ACCELERATOR_NAME).raw.trace``

The executable ``$(NAME).emul.elf`` and the "raw" trace ``$(ACCELERATOR_NAME).raw.trace``
will be used later in `predictDataCycle` and `predictInstrCycle`.

.. code-block:: make

	# insert print statement at the end of each BB in the IR generated by predictLoweringLinking
	predictInsertTrack:
		$(LLVM_HOME)opt $(OPT_FLAGS) -legup-track-bb < $(NAME).sw.bc > $(NAME).track_bb.bc
		$(LLVM_HOME)llvm-dis $(NAME).track_bb.bc

`predictInsertTrack` runs a pass ``-legup-track-bb`` on top of the complete
but maniputated IR ``$(NAME).sw.bc``. In the new IR ``$(NAME).track_bb.bc``,
the print statements for tracking purpose, are inserted in front of all the call,
return instructions, and at end of the basic blocks.

.. code-block:: make

		\
		hybridSwOnlyPass \
		hybridCompileCwrapper \
		hybridSWloweringLinking \
		hybridMIPSbackend \
		hybridMIPSbinUtils \

`hybridSwOnlyPass`, `hybridCompileCwrapper`, `hybridSWloweringLinking`,
`hybridMIPSbackend` and `hybridMIPSbinUtils` are used in regular hybrid flow.
In prediction flow, we run these five targets to get the same SW-side executable
``$(NAME).elf`` in regular hybrid flow. As same as ``$(NAME).emul.elf``,
``$(NAME).elf`` will also be used in `predictDataCycle` and `predictInstrCycle`. 

.. code-block:: make

	#compile HW IR to Verilog backend for hybrid
	hybridVerilogbackend:
		export LEGUP_ACCELERATOR_FILENAME=$(NAME); \
		$(LLVM_HOME)llc -legup-config=config.tcl -legup-config=parallelaccels.tcl $(LLC_FLAGS) -march=v $(NAME).hw.bc -o $(VFILE)
		cp $(NAME).v $(PWD)/tiger/
		#only move .mif files if it exists 
		find . -maxdepth 1 -name "\*.mif" -print0 | xargs -0 -I {} mv {} ./tiger	

`hybridVerilogbackend` is also used in regular hybrid flow to run backend HLS
for HW-side IR. The purpose of running this target in prediction flow is to
gather scheduling information of the hardware. Here, the scheduing information
is generated based on the renamed version of HW-side IR and saved in
``scheduling.legup.rpt``.

.. code-block:: make

	# after the accelerating function is synthesize to HW, this target combine scheduling information and BB trace and reports HW cycle of accelerator
	predictStateTrace:
		# interpret IR that is generated by predictInsertTrack
		$(LLVM_HOME)lli $(NAME).track_bb.bc | grep 'Track@' | sed 's/Track@//' > $(ACCELERATOR_NAME).lli_bb.trace
		# combime the BB trace and scheduing information of BBs in order to get HW cycle
		perl $(PROF_TOOLS)/../partition_analysis/get_hw_cycle.pl $(ACCELERATOR_NAME).lli_bb.trace $(ACCELERATOR_NAME).acel_cycle.rpt
		cat $(ACCELERATOR_NAME).acel_cycle.rpt

`predictStateTrace` is used to estimate HW cycle. It uses LLVM lli command to
interpret ``($NAME).track_bb.bc`` which has print statements inserted. Output is
redirected to ``$(ACCELERATOR_NAME).lli_bb.trace``. Then a perl script ``get_hw_cycle.pl``
takes the trace as input, based on the scheduled lengthes of BBs in ``legup.scheduling.rpt``
and reports HW cycles of accelrating function. Result is saved in
``$(ACCELERATOR_NAME).acel_cycle.rpt``.

.. code-block:: make

	# predict number of cycles spent on load/store from/to global memory
	predictDataCycle:
		# get the global variables that will be accessed by accelerator
		grep '^@' $(NAME).hw.ll | sed 's/^@//' | sed 's/ .*//' > $(ACCELERATOR_NAME).hw_accessed_gv.src
		# get the names, sizes and starting addresses of global variables from the pure SW src code
		$(MIPS_PREFIX)objdump -t $(NAME).emul.elf | grep '\s\.scommon\s\|\s\.rodata\s\|\s\.bss\s\|\s\.sbss\s\|\s\.data\s' | grep -v '^[0-9a-e]\+ l\s\+d\s' | sort -k 1.9 > $(ACCELERATOR_NAME).emul.gv_table.src
		# get the names, sizes and starting addresses of global variables from the hybrid src code
		$(MIPS_PREFIX)objdump -t $(NAME).elf      | grep '\s\.scommon\s\|\s\.rodata\s\|\s\.bss\s\|\s\.sbss\s\|\s\.data\s' | grep -v '^[0-9a-e]\+ l\s\+d\s' | sed 's/ \.scommon\t/ \.bss\t/' | sort -k 1.9  > $(ACCELERATOR_NAME).gv_table.src
		# extract the traces of loads and stores from raw trace
		perl $(PROF_TOOLS)/../partition_analysis/extract_trace.pl $(NAME).emul.src $(ACCELERATOR_NAME).raw.trace $(ACCELERATOR_NAME).extracted.load.trace $(ACCELERATOR_NAME).extracted.store.trace
		# convert loading address from the gxemul src code to tiger src code
		perl $(PROF_TOOLS)/../partition_analysis/convert_ld_addr.pl $(ACCELERATOR_NAME).hw_accessed_gv.src $(ACCELERATOR_NAME).gv_table.src $(ACCELERATOR_NAME).extracted.load.trace $(ACCELERATOR_NAME).converted.load.trace
		# remove the addresses of Global CONST and local stack
		perl $(PROF_TOOLS)/../partition_analysis/gen_ld_addr.pl $(ACCELERATOR_NAME) $(ACCELERATOR_NAME).converted.load.trace $(ACCELERATOR_NAME).ld_addr.trace 
		# run cache simulation
		../$(LEVEL)/tiger/cache_simulator/cache_sim -file $(ACCELERATOR_NAME).ld_addr.trace -cachesize 8 -ways 1 -linesize 16 -replacementpolicy LRU -prefetch 0 > $(ACCELERATOR_NAME).data_cache.rpt
		# report store cycle
		perl $(PROF_TOOLS)/../partition_analysis/get_store_cycle.pl $(ACCELERATOR_NAME) $(ACCELERATOR_NAME).extracted.store.trace $(ACCELERATOR_NAME).data_store.rpt

`predictDataCycle` is used to estimate number of cycles spent on load/store
from/to global memory. The first line simply greps names of all the global
variables from HW-side IR and save them in ``$(ACCELERATOR_NAME).hw_accessed_gv.src``.
(These GV are visible by HW-side IR. In other words, if a global variable is only
accessed by passing-in address pointer as argument of the accelerating function,
this global variable will not appear in ``hw_accessed_gv`` file. In our example
testbenches, some global constants are accessed by passing-in address pointers as
function arguments. These memory accesses should not be eliminated although they are
global constants.)
Also, the actual addresses of global variables in hybrid system are dumped out
from ``$(NAME).elf`` and saved in ``$(ACCELERATOR_NAME).gv_table.src``. The script
``extract_trace.pl`` extracts out loads and stores from the previously generated
"raw" trace, calculates values of sp in processor when functions are just called,
and saves traces seperately in ``$(ACCELERATOR_NAME).extracted.store.trace`` and
``$(ACCELERATOR_NAME).extracted.load.trace``. To estimate load cycles,
``convert_ld_addr.pl`` firstly converts loading address from "GXemul" version to
the actual "Tiger" version. In this step, all the loads to the global constants
that are visible by HW-side IR will be marked with ``<CONST>``. By elimating the
loads of global constants and local stacks, the script ``gen_ld_addr.pl`` generates
an address trace which can be treated as a complete trace of all the loads from
data cache by both processor and accelerator in actual hybrid system. In this final
verion of load trace ``$(ACCELERATOR_NAME).ld_addr.trace``, the accelerator
sections are marked with ``<Accelerator Started>`` at the begining and ``<Accelerator
Finished>`` at the end. A cache simulator ``cache_sim`` reads the trace and reports
the numbers of hits and misses by accelerator. Also, based on the extraced trace,
the script ``get_store_cycle.pl`` ignores the stores towards the address space on
local stack, and reports the number of stores towards global memory. (Since the
data cache in current hybrid system adopts write-through policy, all the data
stores will need to access global memory, and update ``but not replace`` the
content in data cache.)
 
.. code-block:: make

	# predict number of cycles spent on fetching instructions
	predictInstrCycle:
		# get the addresses of function from the pure SW src code
		$(MIPS_PREFIX)objdump -t $(NAME).emul.elf | grep '\s\.text\s' | grep -v '\.text$$' | sort -k 1.9 > $(ACCELERATOR_NAME).emul.func_table.src
		# get the addresses of function from the hybrid src code
		$(MIPS_PREFIX)objdump -t $(NAME).elf      | grep '\s\.text\s' | grep -v '\.text$$' | sort -k 1.9 > $(ACCELERATOR_NAME).func_table.src
		# convert the instruction address and replace the accelerating function with wrapper function
		perl $(PROF_TOOLS)/../partition_analysis/gen_instr_trace.pl $(ACCELERATOR_NAME) $(ACCELERATOR_NAME).emul.func_table.src $(ACCELERATOR_NAME).func_table.src $(ACCELERATOR_NAME).raw.trace $(ACCELERATOR_NAME).instr_addr.trace
		# run cache simulation
		../$(LEVEL)/tiger/cache_simulator/cache_sim -file $(ACCELERATOR_NAME).instr_addr.trace -cachesize 8 -ways 1 -linesize 16 -replacementpolicy LRU -prefetch 0 > $(ACCELERATOR_NAME).instr_cache.rpt

`predictInstrCycle` is to find out the number cycles spent on fetching instructions
of wrapper function (Since the instructions in wrapper functions are generally
simple, most of the cycles spent in wrapper function are to fetch instruction
and to store arguments to accelerator). The first two lines dump out the function
names and starting addresses from both "Tiger" and "GXemul" versions of exetables.
``gen_instr_trace.pl`` takes the "raw" trace and both function tables as input,
translates instruction addresses from "GXemul" version to "Tiger" version (actual
hybrid system version), substitutes the accelerated section with the instruction
addresses of the wrapper function, and marks the accelerator section.
The produced instruction trace ``$(ACCELERATOR_NAME).instr_addr.trace`` is expected
to be the same as that in the actual hybrid system. Now, a cache simulator can be
used to estimate the number of instruction hits and misses when executing wrapper
function.

.. code-block:: make

	# collect prediction data
	predictCollectData:
		perl $(PROF_TOOLS)/../partition_analysis/collect_prediction_data.pl $(NAME) $(ACCELERATOR_NAME) $(NAME).hybrid_prediction.csv

`predictCollectData` will parse out all parts of prediction data from previously
generated report files and save the final prediction result in ``$(NAME).hybrid_
prediction.csv`` and ``$(NAME).hybrid_prediction.rpt``. Both files will be either
created or appended.


To run a complete partition analysis of a program, ``predictAll`` is created to
profile the C program's execution in pure SW mode and estimate the hybrid cycles
of the functions that take more than 5% of hierarchical runtime (includeing
runtime of descendant functions) in SW.

